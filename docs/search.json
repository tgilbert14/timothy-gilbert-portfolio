[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "R√©sum√©",
    "section": "",
    "text": "You can download the my R√©sum√©/CV: CLICK ME!"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "R√©sum√©",
    "section": "Education",
    "text": "Education\n\nUniversity of Arizona\nB.S. Natural Resources: Wildlife Conservation & Management (May 2017)"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "R√©sum√©",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nIT Computing Analyst III\nUniversity of Arizona ‚Äì VGS Project | Tucson, AZ\nNov 2022 ‚Äì Present\n- Developed and deployed R Shiny applications with SQL backends for research workflows\n- Automated Excel-to-SQL data ingestion with built-in QA/QC\n- Collaborated with USFS, NRCS, BLM, BIA on environmental data training and support\n- Managed local & server-side databases (SQL Server, SQLite) and mentored student workers\n\n\nIT Support Computing Analyst I\nUniversity of Arizona ‚Äì VGS Project | Tucson, AZ\nMar 2022 ‚Äì Nov 2022\n- Supported cross-platform VGS troubleshooting and user training\n- Built R-based tools for database manipulation and field reporting\n(See R√©sum√©/CV for complete work history)"
  },
  {
    "objectID": "resume.html#programming-experience",
    "href": "resume.html#programming-experience",
    "title": "R√©sum√©",
    "section": "Programming Experience",
    "text": "Programming Experience\n\nVGS Batch Importer App (R Shiny, DBI, RSQLite)\n\nNEON Small Mammal Tracker (leaflet, Shiny)\n\nNEON Water Chemistry Viewer (plotly, ML predictions)\n\nElectron-Bundled Shiny Desktop App (Electron, Shiny)\n\nNCAA Recruitment Trend Visualizer (Python + R)"
  },
  {
    "objectID": "resume.html#skills-expertise",
    "href": "resume.html#skills-expertise",
    "title": "R√©sum√©",
    "section": "Skills & Expertise",
    "text": "Skills & Expertise\n\nTechnical\nR (Shiny, tidyverse, ggplot2, DBI), SQL (Server, SQLite), Python (pandas, web scraping), GIS (ArcMap, Survey123), QA/QC automation, Git & GitHub, Quarto\n\n\nField & Ecological\nAquatic sampling, small mammal & entomology surveys, vegetation monitoring, soil & microbial sampling, 4√ó4 field driving"
  },
  {
    "objectID": "resume.html#references",
    "href": "resume.html#references",
    "title": "R√©sum√©",
    "section": "References",
    "text": "References\nAvailable upon request or view at the bottom of the R√©sum√©/CV PDF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Who Am I?",
    "section": "",
    "text": "Ecologist ¬∑ Data Scientist ¬∑ Data Analyst ¬∑ IT Support Specialist\nWelcome to my digital portfolio! I specialize in building intuitive data applications and workflows that connect field-based ecological expertise that translate ecological fieldwork into practical tools for environmental analysis and decision-making. I also love web scraping data to create data visualizations and creating forecasting tools."
  },
  {
    "objectID": "index.html#currently-exploring",
    "href": "index.html#currently-exploring",
    "title": "Who Am I?",
    "section": "Currently Exploring",
    "text": "Currently Exploring\nI am always working on developing new skills, here are some of the things I am focusing on‚Ä¶\n\nCustom Web Scraping interactive tools\nVisual storytelling with Quarto + ggplot2 for Big 12 recruiting\nData entry applications with SQL back-end\nPython applications\nMachine learning for forecasting\n\nIf you‚Äôre a researcher, developer, or agency looking for some R incite or to build interactive R shiny data applications, please reach out! I am always looking for new ecological projects to work on and develop my skills."
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Who Am I?",
    "section": "Contact Me",
    "text": "Contact Me\n\nüìß Email Me \n üîó My GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I‚Äôm Timothy, an ecologist turned data scientist with a passion for building tools that drive a deeper understanding of data, particularly when it comes to conservation and the environment. My journey began with ecological data collection from small mammal trapping, electro-fishing, to plant diversity surveys evolving into designing SQL database schemas, predictive dashboards, data management and QA systems to make intriguing and insightful data visualizations.\nI love wrangling messy ecological field data, designing visual workflows for conservation agencies, and creating SQL databases to store long term data collection. I‚Äôve worked with federal agencies, research institutions, and cross-functional technical teams and am always aiming to improve and create new projects!\nI believe good data tools should be intuitive, adaptable, and easy to understand... Most recently, I‚Äôve been exploring Big 12 recruiting data for basketball and football programs and creating plot visualizations via R.\nOh, I also love ultra running and long hikes with my pups üêïüêï"
  },
  {
    "objectID": "about.html#interests-focus",
    "href": "about.html#interests-focus",
    "title": "About Me",
    "section": "üåø Interests & Focus üåø",
    "text": "üåø Interests & Focus üåø\n\nEcological data pipelines and interactive tools\nR Shiny dashboards, SQL databases, and QA automation\nVisual storytelling through data (ggplot2, Quarto, Plotly, Leaflet)\nData visualizations of recruit classes from 247 sports (along with web scraping)"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let‚Äôs Connect!",
    "text": "Let‚Äôs Connect!\nüìß tsgilbert@arizona.edu\nüíæ GitHub"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Apps, Dashboards & Visualizations",
    "section": "",
    "text": "Welcome to a curated gallery of my favorite data stories ‚Äî here are some of the interactive projects I‚Äôve enjoyed building and hosting!\n\n\n\n\nUofA Football Recruiting Pipeline (2023-2026) (Leaflet + HTML + Geocoder) ‚û°Ô∏è\n\n\n\n\n\n\nMammal Tracker (Shiny + Leaflet + GGMap) ‚û°Ô∏è\n\n\n\n\n\n\nClient Data Manipulation (Shiny + ReadXL + Openxlsx) (username=1, password=1) ‚û°Ô∏è\n\n\n\n\n\n\nWater Chemistry Viewer (Shiny + Plotly + MLR) ‚û°Ô∏è\n\n\n\n\n\n\nUSFS Name Converter (Shiny + DT) ‚û°Ô∏è"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "These are some of the projects I‚Äôve built to show the type of tools I love creating!"
  },
  {
    "objectID": "projects.html#ncaa-recruitment-trend-visualizer",
    "href": "projects.html#ncaa-recruitment-trend-visualizer",
    "title": "My Projects",
    "section": "NCAA Recruitment Trend Visualizer",
    "text": "NCAA Recruitment Trend Visualizer\nThis project scrapes recruiting data from multiple sports sites (like 247Sports and On3) to visualize University of Arizona football and basketball class rankings along with other schools via scraping the web. It uses custom R scripts to pull, clean, and plot athlete data ‚Äî giving a quick snapshot of how UA stacks up across seasons. The visualizations are built with ggplot2 and exported as clean, shareable graphics. Great for fans, analysts, or anyone curious about recruiting trends.\n\nCode: GitHub Repo\nExamples:\n\nInteractive UofA Football Recruiting Pipeline Leaflet Map (2023-2026) [Click Me]\nBig 12 College Basketball Recruiting Class Grades from 2016-2025 ‚¨áÔ∏è\n\nUniversity of Arizona vs ASU Basketball Recruit Comparisons ‚¨áÔ∏è"
  },
  {
    "objectID": "projects.html#vgs-batch-importer",
    "href": "projects.html#vgs-batch-importer",
    "title": "My Projects",
    "section": "VGS-Batch-Importer",
    "text": "VGS-Batch-Importer\nThis R Shiny application was built as an ETL pipeline to ingest batch historical excel datasheets into a SQLite database. Generic Data Sheets were build with QA validations built in. This example allows for site, point ground cover, line intercept, and nested frequency data for multiple transects to be ingested for multiple sheets at a time.\nThis app is specifically set to connect to a local VGS database. It can be adapted to be used for a variety of different digital data sheets that contain historical data in different formats. Basic structure works like so; reads data, parses data based on keys set in script (e.g., ‚ÄúSiteID‚Äù to find site name) and selected inputs from UI, organizes data, connects to local SQL database, creates and executes SQL insert statements to create site, site metadata, locations, protocols for event data (includes event groups and events in each eventgroup), then inserts sample data into each event. Various QA/QC checks are built in to prevent corrupt data from being inserted. Code can be seen at https://github.com/tgilbert14/VGS-Batch-Importer-App\n\nCode: GitHub Repo\n\n\nApplications Details ‚¨áÔ∏è\n\n\n\nMain UI View\n\n\nThe app interface has options to select protocols available for ingest. It has various options depending on the import:\n\nPower Mode: This check box bypasses errors by generating and opening a excel workbook to review. The errors are then fixed and eventually the import happens with this setting turned off.\nSpecies Replace: This check box enables a SpeciesReplace.xlsx file that can be used to mass update species codes for every file instead of going into each individual file and changing it individually. This is exceptionally helpful when a USDA code has had an update or a client uses the wrong code consistency.\nSelect Protocol for Import: Sampling protocols (quanitative data collection through VGS) are hard-coded for the selection drop down list. Surveys (qualative data collection through VGS) were designed later and query the local VGS database on your local machine to offer drop down selections for what surveys are available on the device to import data to.\nBatch Import Data: This prompts a pop up window to select the batch import files to import (multiple .xlsx files can be imported at once).\nSpecies Count: This button queries that VGS database and counts the species at each site to provide an overview of the data that was collected.\nSpecies Check: This button brings a pop up that lets the client select a list of states that the sites are located. It then compares the plant codes in the database to USDA plant lists by each state (www/sp_lists_USDA) to check for inconsistencies and data entry errors.\nUpdate Site Name: This links to a script that looks through all location lat/long coordinates and check them in a USFS enterprise shape file to help predict what folder (Allotment/Pasture) they belong in and rename them according to USFS naming conventions (numeric based off of Region-Forest-Ranger District-Allotment-Pasture-SiteID).\nSurvey Log Input?: This new feature offers survey (qualitative data) import with pre-built surveys. This section is still under development.\nü¶ê: This is a help button that provides general workflow for batch importing data with this app.\n\n\nPower Mode ‚è¨"
  },
  {
    "objectID": "projects.html#section",
    "href": "projects.html#section",
    "title": "My Projects",
    "section": "",
    "text": "Species Replace ‚è¨"
  },
  {
    "objectID": "projects.html#section-1",
    "href": "projects.html#section-1",
    "title": "My Projects",
    "section": "",
    "text": "Species Count ‚è¨"
  },
  {
    "objectID": "projects.html#section-2",
    "href": "projects.html#section-2",
    "title": "My Projects",
    "section": "",
    "text": "Species Check ‚è¨"
  },
  {
    "objectID": "projects.html#section-3",
    "href": "projects.html#section-3",
    "title": "My Projects",
    "section": "",
    "text": "This app also generates a log text file stored in the ‚Äòwww/‚Äô folder to track code flow and debug import issues. ‚è¨\n\n\n\nwww/r_output.txt log"
  },
  {
    "objectID": "projects.html#neon-small-mammal-tracker",
    "href": "projects.html#neon-small-mammal-tracker",
    "title": "My Projects",
    "section": "NEON Small Mammal Tracker",
    "text": "NEON Small Mammal Tracker\nThis Shiny application visualizes small mammal capture data from the National Ecological Observatory Network (NEON). Users can select a location and date range to compare captures across NEON sites. The app processes individual ID tags to rank sites by total capture volume, offering a clear view of spatial and temporal patterns in small mammal activity.\n\nCode: GitHub Repo\nLive App: RatTrapHistoryApp"
  },
  {
    "objectID": "projects.html#neon-water-chemistry-viewer",
    "href": "projects.html#neon-water-chemistry-viewer",
    "title": "My Projects",
    "section": "NEON Water Chemistry Viewer",
    "text": "NEON Water Chemistry Viewer\nThis Shiny app lets you explore NEON‚Äôs Surface Water Chemistry (SWC) data across ecological sites in the U.S. Just pick a date range, aquatic site, and two analytes to compare ‚Äî then hit ‚ÄúProcess Selection(s)‚Äù to see how they stack up. It‚Äôs built to help visualize water chemistry shifts over time and across space. Curious about how the data‚Äôs collected? Click on any SWC label in the app, or learn more about NEON‚Äôs mission here.\n\nCode: GitHub Repo\nLive App: WaterAnalyteApp"
  },
  {
    "objectID": "projects.html#electron-bundled-shiny-desktop-app",
    "href": "projects.html#electron-bundled-shiny-desktop-app",
    "title": "My Projects",
    "section": "Electron-Bundled Shiny Desktop App",
    "text": "Electron-Bundled Shiny Desktop App\nA standalone desktop tool embedding a Shiny UI with an internal SQL database back-end. This local Shiny app is built to manage structured data offline with a bundled SQL database ‚Äî no cloud required. It‚Äôs designed for research teams, data loggers, or anyone who wants quick access to stored info without depending on external servers. You can insert, update, and retrieve records through a clean R-powered interface, and because it‚Äôs packaged with Electron and R-Portable, it runs as a standalone Windows app ‚Äî even on machines without R installed.\n\nCode: GitHub Repo\n\n\n\n\nContact me @ tsgilbert@arizona.edu for questions, feedback, suggestions, or if you want to collaborate!"
  }
]