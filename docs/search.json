[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "R√©sum√©/CV",
    "section": "",
    "text": "University of Arizona ‚Äì VGS Project | Tucson, AZ\nMarch 2022‚ÄìPresent / Promoted to Analyst III in Nov of 2022 / Full-Time\n\nDeveloped and deployed R Shiny applications for research data workflows, including data ingestion, management, manipulation, quality assurance, and interactive visualizations\nDesigned ETL pipelines for historical ecological datasets, parsing Excel files into structured SQL tables with QA/QC protocols built in\nMaintained local and server-side databases (Microsoft SQL Server, SQLite), automated SQL scripts for data quality and reporting\nCollaborated with federal agencies, universities and other non-profits on environmental data collection and technical support including maintenance of field data collection devices\nLed and mentored a student data entry team, standardizing QA protocols for high-volume historical data transcription\nSupported cross-platform software testing (Windows, iOS, Android) in a C# environment using Visual Studio, Visual Studio Code; tracked bugs and fixes via Jira\nSupported agile development workflows and sprint tasks across technical and field teams; led daily scrum meetings\nAssisted in preparing quarterly reports for federal funding sources, documenting project progress, deliverables, and achievements"
  },
  {
    "objectID": "resume.html#programming-projects",
    "href": "resume.html#programming-projects",
    "title": "R√©sum√©/CV",
    "section": "Programming Projects",
    "text": "Programming Projects\nMY PROJECTS"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "R√©sum√©/CV",
    "section": "Education",
    "text": "Education\n\nUniversity of Arizona\nB.S. Natural Resources: Wildlife Conservation & Management (May 2017)"
  },
  {
    "objectID": "resume.html#references",
    "href": "resume.html#references",
    "title": "R√©sum√©/CV",
    "section": "References",
    "text": "References\nAvailable upon request or view at the bottom of the R√©sum√©/CV PDF.\n\nYou can download the my R√©sum√©/CV: CLICK ME!\n\n&lt;p&gt;This browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"assets/Gilbert_ResumeCV.pdf\"&gt;Download R√©sum√©&lt;/a&gt;.&lt;/p&gt;\n\n\n\n  ABOUT ME MY PROJECTS RESUME/CV SHINY APPS"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Who Am I?",
    "section": "",
    "text": "Ecologist ¬∑ Data Scientist ¬∑ Data Analyst ¬∑ IT Support Specialist\nWelcome to my digital portfolio! I specialize in building intuitive data applications and workflows that connect field-based ecological expertise that translate ecological fieldwork into practical tools for environmental analysis and decision-making. I also love web scraping data to create data visualizations and creating forecasting tools."
  },
  {
    "objectID": "index.html#currently-exploring",
    "href": "index.html#currently-exploring",
    "title": "Who Am I?",
    "section": "Currently Exploring",
    "text": "Currently Exploring\nI am always working on developing new skills, here are some of the things I am focusing on‚Ä¶\n\nCustom Web Scraping interactive tools\nVisual storytelling with Quarto + ggplot2 for Big 12 recruiting\nData entry applications with SQL back-end\nPython applications\nMachine learning for forecasting\n\nIf you‚Äôre a researcher, developer, or agency looking for some R incite or to build interactive R shiny data applications, please reach out! I am always looking for new ecological projects to work on and develop my skills."
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Who Am I?",
    "section": "Contact Me",
    "text": "Contact Me\n\nüìß Email Me \n üîó My GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I‚Äôm Timothy, an ecologist turned data scientist with a passion for building tools that drive a deeper understanding of data, particularly when it comes to conservation and the environment. My journey began with ecological data collection from small mammal trapping, electro-fishing, to plant diversity surveys evolving into designing SQL database schemas, predictive dashboards, data management and QA systems to make intriguing and insightful data visualizations.\nI love wrangling messy ecological field data, designing visual workflows for conservation agencies, and creating SQL databases to store long term data collection. I‚Äôve worked with federal agencies, research institutions, and cross-functional technical teams and am always aiming to improve and create new projects!\nI believe good data tools should be intuitive, adaptable, and easy to understand... Most recently, I‚Äôve been exploring Big 12 recruiting data for basketball and football programs and creating plot visualizations via R.\nOh, I also love the tucson dry heat, long hikes with my pups and ultra running!"
  },
  {
    "objectID": "about.html#interests-focus",
    "href": "about.html#interests-focus",
    "title": "About Me",
    "section": "üåø Interests & Focus üåø",
    "text": "üåø Interests & Focus üåø\n\nEcological data pipelines and interactive tools\nR Shiny dashboards, SQL databases, and QA automation\nVisual storytelling through data (ggplot2, Quarto, Plotly, Leaflet)\nData visualizations of recruit classes from 247 sports (along with web scraping)"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let‚Äôs Connect!",
    "text": "Let‚Äôs Connect!\nüìß tsgilbert@arizona.edu\nüíæ GitHub"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Apps, Dashboards & Visualizations",
    "section": "",
    "text": "Welcome to a curated gallery of my favorite data stories, here are some of the interactive projects I‚Äôve enjoyed building and hosting!\n\n\n\nNCAA Big 12 Recruiting Pipeline (shiny + geocoder + leaflet) ‚û°Ô∏è\n\n\n\n\nMammal Tracker (shiny + leaflet + ggmap) ‚û°Ô∏è\n\n\n\n\nClient Data Manipulation (shiny + readxl + openxlsx) *username=1, password=1 ‚û°Ô∏è\n\n\n\n\n\n\n\nUSFS Name Converter (shiny + tidyverse + DT) ‚û°Ô∏è\n\n\n\n\n\n\n  ABOUT ME MY PROJECTS RESUME/CV SHINY APPS"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "These are some of the projects I‚Äôve built to show the type of tools I love creating!"
  },
  {
    "objectID": "projects.html#ncaa-recruitment-trend-visualizer",
    "href": "projects.html#ncaa-recruitment-trend-visualizer",
    "title": "My Projects",
    "section": "NCAA Recruitment Trend Visualizer",
    "text": "NCAA Recruitment Trend Visualizer\nScrapes recruiting data from multiple sports sites (e.g., 247Sports, On3) to visualize Big 12 football and basketball class rankings. Custom R scripts handle data extraction, cleaning, and plotting, with interactive controls for sport, class year, and team. Built in Shiny with Leaflet for dynamic geospatial visualizations.\n\nCode: GitHub\nLive App: App\n\n‚≠Ü Big 12 College Basketball Recruiting Class Grades from 2016-2025 ‚¨áÔ∏è\n\n‚≠Ü University of Arizona vs ASU Basketball Recruit Comparisons ‚¨áÔ∏è"
  },
  {
    "objectID": "projects.html#vgs-batch-importer",
    "href": "projects.html#vgs-batch-importer",
    "title": "My Projects",
    "section": "VGS Batch Importer",
    "text": "VGS Batch Importer\nAn ETL pipeline built in R to ingest historical Excel datasheets into a SQLite database. Supports multiple vegetation protocols (e.g., point ground cover, line intercept, nested frequency) across transects and sites. The app parses data using key identifiers (e.g., SiteID), organizes metadata, and inserts structured records into a local VGS database. QA/QC checks are embedded to prevent corrupt or incomplete data from being ingested.\n\nCode: GitHub\nLive App: Local-only Shiny app\n\n\nApplications Details ‚¨áÔ∏è\n\n\n\nMain UI View\n\n\nThe app interface has options to select protocols available for ingest. It has various options depending on the import:\n\nPower Mode: This check box bypasses errors by generating and opening a excel workbook to review. The errors are then fixed and eventually the import happens with this setting turned off.\nSpecies Replace: This check box enables a SpeciesReplace.xlsx file that can be used to mass update species codes for every file instead of going into each individual file and changing it individually. This is exceptionally helpful when a USDA code has had an update or a client uses the wrong code consistency.\nSelect Protocol for Import: Sampling protocols (quanitative data collection through VGS) are hard-coded for the selection drop down list. Surveys (qualative data collection through VGS) were designed later and query the local VGS database on your local machine to offer drop down selections for what surveys are available on the device to import data to.\nBatch Import Data: This prompts a pop up window to select the batch import files to import (multiple .xlsx files can be imported at once).\nSpecies Count: This button queries that VGS database and counts the species at each site to provide an overview of the data that was collected.\nSpecies Check: This button brings a pop up that lets the client select a list of states that the sites are located. It then compares the plant codes in the database to USDA plant lists by each state (www/sp_lists_USDA) to check for inconsistencies and data entry errors.\nUpdate Site Name: This links to a script that looks through all location lat/long coordinates and check them in a USFS enterprise shape file to help predict what folder (Allotment/Pasture) they belong in and rename them according to USFS naming conventions (numeric based off of Region-Forest-Ranger District-Allotment-Pasture-SiteID).\nSurvey Log Input?: This new feature offers survey (qualitative data) import with pre-built surveys. This section is still under development.\nü¶ê: This is a help button that provides general workflow for batch importing data with this app.\n\n\nPower Mode ‚è¨"
  },
  {
    "objectID": "projects.html#section",
    "href": "projects.html#section",
    "title": "My Projects",
    "section": "",
    "text": "Species Replace ‚è¨"
  },
  {
    "objectID": "projects.html#section-1",
    "href": "projects.html#section-1",
    "title": "My Projects",
    "section": "",
    "text": "Species Count ‚è¨"
  },
  {
    "objectID": "projects.html#section-2",
    "href": "projects.html#section-2",
    "title": "My Projects",
    "section": "",
    "text": "Species Check ‚è¨"
  },
  {
    "objectID": "projects.html#section-3",
    "href": "projects.html#section-3",
    "title": "My Projects",
    "section": "",
    "text": "This app also generates a log text file stored in the ‚Äòwww/‚Äô folder to track code flow and debug import issues. ‚è¨\n\n\n\nwww/r_output.txt log"
  },
  {
    "objectID": "projects.html#neon-small-mammal-tracker",
    "href": "projects.html#neon-small-mammal-tracker",
    "title": "My Projects",
    "section": "NEON Small Mammal Tracker",
    "text": "NEON Small Mammal Tracker\nInteractive Shiny app visualizing small mammal capture data from the National Ecological Observatory Network (NEON). Users select location and date range to compare capture volumes across NEON sites. The app processes individual ID tags to rank sites by total captures, revealing spatial and temporal trends in mammal activity.\n\nCode: GitHub\nLive App: App"
  },
  {
    "objectID": "projects.html#neon-water-chemistry-viewer",
    "href": "projects.html#neon-water-chemistry-viewer",
    "title": "My Projects",
    "section": "NEON Water Chemistry Viewer",
    "text": "NEON Water Chemistry Viewer\nThis Shiny app lets you explore NEON‚Äôs Surface Water Chemistry (SWC) data across ecological sites in the U.S. Just pick a date range, aquatic site, and two analytes to compare, then hit ‚ÄúProcess Selection(s)‚Äù to see how they stack up. It‚Äôs built to help visualize water chemistry shifts over time and across space. Curious about how the data‚Äôs collected? Click on any SWC label in the app, or learn more about NEON‚Äôs mission here.\n\nCode: GitHub\nLive App: App"
  },
  {
    "objectID": "projects.html#electron-bundled-shiny-desktop-app",
    "href": "projects.html#electron-bundled-shiny-desktop-app",
    "title": "My Projects",
    "section": "Electron-Bundled Shiny Desktop App",
    "text": "Electron-Bundled Shiny Desktop App\nA standalone desktop tool combining a Shiny UI with an embedded SQL database. Designed for offline use by research teams and field data loggers, it enables structured data entry, updates, and retrieval without cloud dependencies. Built with Electron and R-Portable, the app runs as a self-contained Windows executable, no R installation required.\n\nCode: GitHub\nLive App: Local-only Shiny app\n\n\n\n\n  ABOUT ME MY PROJECTS RESUME/CV SHINY APPS\n\n\nContact me @ tsgilbert@arizona.edu for questions, feedback, suggestions, or if you want to collaborate!"
  }
]