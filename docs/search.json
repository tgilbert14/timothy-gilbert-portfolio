[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "R√©sum√©",
    "section": "",
    "text": "You can download the my R√©sum√©/CV: CLICK ME!"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "R√©sum√©",
    "section": "Education",
    "text": "Education\n\nUniversity of Arizona\nB.S. Natural Resources: Wildlife Conservation & Management (May 2017)"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "R√©sum√©",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nIT Computing Analyst III\nUniversity of Arizona ‚Äì VGS Project | Tucson, AZ\nNov 2022 ‚Äì Present\n- Developed and deployed R Shiny applications with SQL backends for research workflows\n- Automated Excel-to-SQL data ingestion with built-in QA/QC\n- Collaborated with USFS, NRCS, BLM, BIA on environmental data training and support\n- Managed local & server-side databases (SQL Server, SQLite) and mentored student workers\n\n\nIT Support Computing Analyst I\nUniversity of Arizona ‚Äì VGS Project | Tucson, AZ\nMar 2022 ‚Äì Nov 2022\n- Supported cross-platform VGS troubleshooting and user training\n- Built R-based tools for database manipulation and field reporting\n(See R√©sum√©/CV for complete work history)"
  },
  {
    "objectID": "resume.html#programming-experience",
    "href": "resume.html#programming-experience",
    "title": "R√©sum√©",
    "section": "Programming Experience",
    "text": "Programming Experience\n\nVGS Batch Importer App (R Shiny, DBI, RSQLite)\n\nNEON Small Mammal Tracker (leaflet, Shiny)\n\nNEON Water Chemistry Viewer (plotly, ML predictions)\n\nElectron-Bundled Shiny Desktop App (Electron, Shiny)\n\nNCAA Recruitment Trend Visualizer (Python + R)"
  },
  {
    "objectID": "resume.html#skills-expertise",
    "href": "resume.html#skills-expertise",
    "title": "R√©sum√©",
    "section": "Skills & Expertise",
    "text": "Skills & Expertise\n\nTechnical\nR (Shiny, tidyverse, ggplot2, DBI), SQL (Server, SQLite), Python (pandas, web scraping), GIS (ArcMap, Survey123), QA/QC automation, Git & GitHub, Quarto\n\n\nField & Ecological\nAquatic sampling, small mammal & entomology surveys, vegetation monitoring, soil & microbial sampling, 4√ó4 field driving"
  },
  {
    "objectID": "resume.html#references",
    "href": "resume.html#references",
    "title": "R√©sum√©",
    "section": "References",
    "text": "References\nAvailable upon request or view at the bottom of the R√©sum√©/CV PDF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Who Am I?",
    "section": "",
    "text": "Ecologist ¬∑ Data Scientist ¬∑ Data Analyst ¬∑ IT Support Specialist\nWelcome to my digital portfolio! I specialize in building intuitive data applications and workflows that connect field-based ecological expertise that translate ecological fieldwork into practical tools for environmental analysis and decision-making. I also love web scraping data to create data visualizations and creating forecasting tools."
  },
  {
    "objectID": "index.html#currently-exploring",
    "href": "index.html#currently-exploring",
    "title": "Who Am I?",
    "section": "Currently Exploring",
    "text": "Currently Exploring\nI am always working on developing new skills, here are some of the things I am focusing on‚Ä¶\n\nCustom Web Scraping interactive tools\nVisual storytelling with Quarto + ggplot2 for Big 12 recruiting\nData entry applications with SQL back-end\nPython applications\nMachine learning for forecasting\n\nIf you‚Äôre a researcher, developer, or agency looking for some R incite or to build interactive R shiny data applications, please reach out! I am always looking for new ecological projects to work on and develop my skills."
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Who Am I?",
    "section": "Contact Me",
    "text": "Contact Me\n\nüìß Email Me \n üîó My GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I‚Äôm Timothy, an ecologist turned data scientist with a passion for building tools that drive a deeper understanding of data, particularly when it comes to conservation and the environment. My journey began with ecological data collection from small mammal trapping, electro-fishing, to plant diversity surveys evolving into designing SQL database schemas, predictive dashboards, data management and QA systems to make intriguing and insightful data visualizations.\nI love wrangling messy ecological field data, designing visual workflows for conservation agencies, and creating SQL databases to store long term data collection. I‚Äôve worked with federal agencies, research institutions, and cross-functional technical teams and am always aiming to improve and create new projects!\nI believe good data tools should be intuitive, adaptable, and easy to understand... Most recently, I‚Äôve been exploring Big 12 recruiting data for basketball and football programs and creating plot visualizations via R.\nOh, I also love ultra running and long hikes with my pups üêïüêï"
  },
  {
    "objectID": "about.html#interests-focus",
    "href": "about.html#interests-focus",
    "title": "About Me",
    "section": "üåø Interests & Focus üåø",
    "text": "üåø Interests & Focus üåø\n\nEcological data pipelines and interactive tools\nR Shiny dashboards, SQL databases, and QA automation\nVisual storytelling through data (ggplot2, Quarto, Plotly, Leaflet)\nData visualizations of recruit classes from 247 sports (along with web scraping)"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let‚Äôs Connect!",
    "text": "Let‚Äôs Connect!\nüìß tsgilbert@arizona.edu\nüíæ GitHub"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Dashboards & Visualizations",
    "section": "",
    "text": "Welcome to a curated gallery of my favorite data stories ‚Äî here are some of the interactive projects I‚Äôve enjoyed building and hosting!"
  },
  {
    "objectID": "dashboards.html#r-shiny-applications",
    "href": "dashboards.html#r-shiny-applications",
    "title": "Dashboards & Visualizations",
    "section": "R Shiny Applications",
    "text": "R Shiny Applications\n\n\nMammal Tracker (Shiny + Leaflet + GGMap) ‚û°Ô∏è\n\n\n\nClient Data Manipulation (Shiny + ReadXL + Openxlsx) (username=1, password=1) ‚û°Ô∏è\n\n\n\nWater Chemistry Viewer (Shiny + Plotly + MLR) ‚û°Ô∏è\n\n\n\nUSFS Name Converter (Shiny + DT) ‚û°Ô∏è"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "These are some of the projects I‚Äôve built to show the type of tools I love creating!"
  },
  {
    "objectID": "projects.html#neon-small-mammal-tracker",
    "href": "projects.html#neon-small-mammal-tracker",
    "title": "My Projects",
    "section": "NEON Small Mammal Tracker",
    "text": "NEON Small Mammal Tracker\nThis Shiny application visualizes small mammal capture data from the National Ecological Observatory Network (NEON). Users can select a location and date range to compare captures across NEON sites. The app processes individual ID tags to rank sites by total capture volume, offering a clear view of spatial and temporal patterns in small mammal activity.\n\nCode: GitHub Repo\nLive App: RatTrapHistoryApp"
  },
  {
    "objectID": "projects.html#neon-water-chemistry-viewer",
    "href": "projects.html#neon-water-chemistry-viewer",
    "title": "My Projects",
    "section": "NEON Water Chemistry Viewer",
    "text": "NEON Water Chemistry Viewer\nThis Shiny app lets you explore NEON's Surface Water Chemistry (SWC) data across ecological sites in the U.S. Just pick a date range, aquatic site, and two analytes to compare ‚Äî then hit \"Process Selection(s)\" to see how they stack up. It's built to help visualize water chemistry shifts over time and across space. Curious about how the data's collected? Click on any SWC label in the app, or learn more about NEON's mission here.\n\nCode: GitHub Repo\nLive App: WaterAnalyteApp"
  },
  {
    "objectID": "projects.html#ncaa-recruitment-trend-visualizer",
    "href": "projects.html#ncaa-recruitment-trend-visualizer",
    "title": "My Projects",
    "section": "NCAA Recruitment Trend Visualizer",
    "text": "NCAA Recruitment Trend Visualizer\nThis project scrapes recruiting data from multiple sports sites (like 247Sports and On3) to visualize University of Arizona football and basketball class rankings along with other schools via scraping the web. It uses custom R scripts to pull, clean, and plot athlete data ‚Äî giving a quick snapshot of how UA stacks up across seasons. The visualizations are built with ggplot2 and exported as clean, shareable graphics. Great for fans, analysts, or anyone curious about recruiting trends.\n\nCode: GitHub Repo"
  },
  {
    "objectID": "projects.html#electron-bundled-shiny-desktop-app",
    "href": "projects.html#electron-bundled-shiny-desktop-app",
    "title": "My Projects",
    "section": "Electron-Bundled Shiny Desktop App",
    "text": "Electron-Bundled Shiny Desktop App\nA standalone desktop tool embedding a Shiny UI with an internal SQL database back-end. This local Shiny app is built to manage structured data offline with a bundled SQL database ‚Äî no cloud required. It‚Äôs designed for research teams, data loggers, or anyone who wants quick access to stored info without depending on external servers. You can insert, update, and retrieve records through a clean R-powered interface, and because it‚Äôs packaged with Electron and R-Portable, it runs as a standalone Windows app ‚Äî even on machines without R installed.\n\nCode: GitHub Repo"
  },
  {
    "objectID": "projects.html#vgs-batch-importer-app",
    "href": "projects.html#vgs-batch-importer-app",
    "title": "My Projects",
    "section": "VGS Batch Importer App",
    "text": "VGS Batch Importer App\nThis Shiny app streamlines batch importing of historical ecological data into a local SQL-backed VGS database. It‚Äôs built to handle messy Excel files with varying structures ‚Äî parsing key fields like SiteID and applying user-selected inputs to organize metadata, create protocol-linked event records, and insert sample data. It‚Äôs adaptable, robust, and designed for real-world field workflows.\nPower Mode flags formatting issues early via QA/QC logs, letting you clean sheets and re-run the pipeline with confidence. Once data is processed, you can generate correct folder names with a script and sync clean datasets to the VGS ecosystem.\n\nCode: GitHub Repo"
  }
]